{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pymorphy2\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import unidecode\n",
    "from keras_preprocessing.text import Tokenizer\n",
    " \n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we load \n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\"G:\\\\New folder\\\\ruwikiruscorpora_tokens_elmo_1024_2019\\\\ruwikiruscorpora_upos_skipgram_300_2_2019\\\\model.bin\", binary=True)\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='холодный', tag=OpencorporaTag('ADJF,Qual masc,sing,nomn'), normal_form='холодный', score=0.5, methods_stack=((<DictionaryAnalyzer>, 'холодный', 86, 0),)),\n",
       " Parse(word='холодный', tag=OpencorporaTag('ADJF,Qual inan,masc,sing,accs'), normal_form='холодный', score=0.5, methods_stack=((<DictionaryAnalyzer>, 'холодный', 86, 4),))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph.parse('холодный')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('лед_NOUN', 0.7199968099594116),\n",
       " ('снежный_ADJ', 0.6809204816818237),\n",
       " ('снег_NOUN', 0.6226842403411865),\n",
       " ('холодный_ADJ', 0.6221423745155334),\n",
       " ('льдистый_ADJ', 0.6192911863327026),\n",
       " ('льдинка_NOUN', 0.6153559684753418),\n",
       " ('сугроб_NOUN', 0.5984904766082764),\n",
       " ('холод_NOUN', 0.594094455242157),\n",
       " ('оледенелый_ADJ', 0.5915398001670837),\n",
       " ('сосулька_NOUN', 0.5900148749351501),\n",
       " ('торос_NOUN', 0.5898441076278687),\n",
       " ('замерзнуть_VERB', 0.5872437953948975),\n",
       " ('иней_NOUN', 0.5848214626312256),\n",
       " ('глыба_NOUN', 0.5659131407737732),\n",
       " ('замерзать_VERB', 0.5654045939445496),\n",
       " ('морозный_ADJ', 0.5636944770812988),\n",
       " ('обледенелый_ADJ', 0.563029408454895),\n",
       " ('студеный_ADJ', 0.5602612495422363),\n",
       " ('снеговой_ADJ', 0.5562052130699158),\n",
       " ('полынья_NOUN', 0.5557663440704346)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(['ледяной_ADJ'], topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_neighbour(word, pos, gend='masc'):\n",
    "    word = word.replace('ё', 'е')\n",
    "    lex = word + '_' + cotags[pos]\n",
    "    if lex in model:\n",
    "        neighbs = model.most_similar([lex], topn=20)\n",
    "        for nei in neighbs:\n",
    "            lex_n, ps_n = nei[0].split('_')\n",
    "            if '::' in lex_n:\n",
    "                continue\n",
    "            if cotags[pos] == ps_n:\n",
    "                if pos == 'NOUN':\n",
    "                    parse_result = morph.parse(lex_n)\n",
    "                    for ana in parse_result:\n",
    "                        if ana.normal_form == lex_n:\n",
    "                            if ana.tag.gender == gend:\n",
    "                                return lex_n\n",
    "                elif cotags[pos] == 'VERB' and word[-2:] == 'ся':\n",
    "                    if lex_n[-2:] == 'ся':\n",
    "                        return lex_n\n",
    "                elif cotags[pos] == 'VERB' and word[-2:] != 'ся':\n",
    "                    if lex_n[-2:] != 'ся':\n",
    "                        return lex_n\n",
    "                else:\n",
    "                    return lex_n\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flection(lex_neighb, tags):\n",
    "    tags = str(tags)\n",
    "    tags = re.sub(',[AGQSPMa-z-]+? ', ',', tags)\n",
    "    tags = tags.replace(\"impf,\", \"\")\n",
    "    tags = re.sub('([A-Z]) (plur|masc|femn|neut|inan)', '\\\\1,\\\\2', tags)\n",
    "    tags = tags.replace(\"Impe neut\", \"\")\n",
    "    tags = tags.split(',')\n",
    "    tags_clean = []\n",
    "    for t in tags:\n",
    "        if t:\n",
    "            if ' ' in t:\n",
    "                t1, t2 = t.split(' ')\n",
    "                t = t2\n",
    "            tags_clean.append(t)\n",
    "    tags = frozenset(tags_clean)\n",
    "    prep_for_gen = morph.parse(lex_neighb)\n",
    "    ana_array = []\n",
    "    for ana in prep_for_gen:\n",
    "        if ana.normal_form == lex_neighb:\n",
    "            ana_array.append(ana)\n",
    "    for ana in ana_array:\n",
    "        try:\n",
    "            flect = ana.inflect(tags)\n",
    "        except:\n",
    "            print(tags)\n",
    "            return None\n",
    "        if flect:\n",
    "            word_to_replace = flect.word\n",
    "            return word_to_replace\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cotags = {'ADJF':'ADJ', # pymorphy2: word2vec \n",
    "'ADJS' : 'ADJ', \n",
    "'ADVB' : 'ADV', \n",
    "'COMP' : 'ADV', \n",
    "'GRND' : 'VERB', \n",
    "'INFN' : 'VERB', \n",
    "'NOUN' : 'NOUN', \n",
    "'PRED' : 'ADV', \n",
    "'PRTF' : 'ADJ', \n",
    "'PRTS' : 'VERB', \n",
    "'VERB' : 'VERB'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "здравствовать\n",
      "здравствуйте\n",
      "здоровать\n",
      "прощай\n",
      "здравствуйтбыть\n",
      "здрасте\n",
      "поцелуться\n",
      "прощать\n",
      "исполать\n",
      "побойся\n",
      "валяй\n",
      "обнимться\n",
      "отвори\n"
     ]
    }
   ],
   "source": [
    "my_word = \"здравствуй\"\n",
    "parsed = morph.parse(my_word)\n",
    "pos = cotags[parsed[0].tag.POS]\n",
    "#find similars\n",
    "gensim_find_word = my_word + \"_\" + pos\n",
    "most_similars = model.most_similar([gensim_find_word], topn=50)\n",
    "#find similars\n",
    "for sim in most_similars:\n",
    "    sim_parsed = sim[0].split(\"_\")\n",
    "    if sim_parsed[1] == pos:\n",
    "        print(sim_parsed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"G:\\\\New folder\\\\month-2011-12-qtraf_small\"\n",
    " \n",
    "text = open(file_path).read()\n",
    "#print(text)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    " \n",
    "encoded = tokenizer.texts_to_sequences([text])[0]\n",
    "#print(encoded) \n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "#print(\"vocab_size %s\" %(vocab_size))\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = tokenizer.index_word\n",
    "#print(\"word_index %s\" %( tokenizer.word_index))\n",
    "#print(\"index_word %s\" %( tokenizer.index_word))\n",
    "\n",
    "sequences = list()\n",
    "\n",
    "for i in range(1, len(encoded)):\n",
    "    sequence = encoded[i - 1:i + 1]\n",
    "    sequences.append(sequence)\n",
    "sequences = np.array(sequences)\n",
    "#print(word2idx)\n",
    "X, Y = sequences[:, 0], sequences[:, 1]\n",
    "X = np.expand_dims(X, 1)\n",
    "Y = np.expand_dims(Y, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 100\n",
    "BATCH_SIZE = 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X, Y)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "embedding_dim = 100\n",
    "units = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 8.4987\n",
      "Epoch 1 Batch 100 Loss 5.2714\n",
      "Epoch 1 Batch 200 Loss 5.3116\n",
      "Epoch 2 Batch 0 Loss 9.0189\n",
      "Epoch 2 Batch 100 Loss 3.4233\n",
      "Epoch 2 Batch 200 Loss 3.8106\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-f1ca6e9fa154>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m             \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    677\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    678\u001b[0m               input_list, self._mixed_precision_policy.should_cast_variables):\n\u001b[1;32m--> 679\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    680\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    681\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-f1ca6e9fa154>\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, hidden)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m#print(inputs)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    667\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mconstants\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'constants'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconstants\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 669\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m   def call(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    677\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    678\u001b[0m               input_list, self._mixed_precision_policy.should_cast_variables):\n\u001b[1;32m--> 679\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    680\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    681\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[0;32m   1905\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_recurrent_dropout_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1906\u001b[0m     return super(GRU, self).call(\n\u001b[1;32m-> 1907\u001b[1;33m         inputs, mask=mask, training=training, initial_state=initial_state)\n\u001b[0m\u001b[0;32m   1908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1909\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, mask, training, initial_state, constants)\u001b[0m\n\u001b[0;32m    741\u001b[0m         \u001b[0minput_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimesteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m         \u001b[0mtime_major\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime_major\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 743\u001b[1;33m         zero_output_for_mask=self.zero_output_for_mask)\n\u001b[0m\u001b[0;32m    744\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m       \u001b[0mupdates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mrnn\u001b[1;34m(step_function, inputs, initial_states, go_backwards, mask, constants, unroll, input_length, time_major, zero_output_for_mask)\u001b[0m\n\u001b[0;32m   3730\u001b[0m     \u001b[1;31m# the value is discarded.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3731\u001b[0m     output_time_zero, _ = step_function(\n\u001b[1;32m-> 3732\u001b[1;33m         input_time_zero, tuple(initial_states) + tuple(constants))\n\u001b[0m\u001b[0;32m   3733\u001b[0m     output_ta = tuple(\n\u001b[0;32m   3734\u001b[0m         tensor_array_ops.TensorArray(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(inputs, states)\u001b[0m\n\u001b[0;32m    726\u001b[0m       \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_tf_rnn_cell\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    729\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m           \u001b[0mnew_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnew_states\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, states, training)\u001b[0m\n\u001b[0;32m   1655\u001b[0m       \u001b[0mrecurrent_z\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh_tm1_z\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecurrent_kernel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1656\u001b[0m       recurrent_r = K.dot(h_tm1_r,\n\u001b[1;32m-> 1657\u001b[1;33m                           self.recurrent_kernel[:, self.units:self.units * 2])\n\u001b[0m\u001b[0;32m   1658\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_after\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1659\u001b[0m         \u001b[0mrecurrent_z\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecurrent_z\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecurrent_bias\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mdot\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1631\u001b[0m     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msparse_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse_tensor_dense_matmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1632\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1633\u001b[1;33m     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1634\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1635\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[1;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[0;32m   2645\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2646\u001b[0m       return gen_math_ops.mat_mul(\n\u001b[1;32m-> 2647\u001b[1;33m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[0;32m   2648\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2649\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[1;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[0;32m   6268\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"MatMul\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6269\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_post_execution_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"transpose_a\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6270\u001b[1;33m         transpose_a, \"transpose_b\", transpose_b)\n\u001b[0m\u001b[0;32m   6271\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6272\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, units, batch_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.units = units\n",
    "        self.batch_size = batch_size\n",
    " \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    " \n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_activation='sigmoid',\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    " \n",
    "    def call(self, inputs, hidden):\n",
    "        inputs = self.embedding(inputs)\n",
    "        #print(inputs)\n",
    "        output, states = self.gru(inputs, initial_state=hidden)\n",
    " \n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    " \n",
    "        x = self.fc(output)\n",
    " \n",
    "        return x, states\n",
    "\n",
    "model = Model(vocab_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    " \n",
    "#checkpoint_dir = '.\\\\training_checkpoints_wordstat'\n",
    "checkpoint_dir = '.\\\\training_checkpoints_wordstat_small2048'\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\n",
    "\n",
    "def loss_function(labels, logits):\n",
    "    return tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    " \n",
    "    hidden = model.reset_states()\n",
    " \n",
    "    for (batch, (input, target)) in enumerate(dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions, hidden = model(input, hidden)\n",
    " \n",
    "            target = tf.reshape(target, (-1,))\n",
    "            loss = loss_function(target, predictions)\n",
    " \n",
    "            grads = tape.gradient(loss, model.variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.variables))\n",
    " \n",
    "            if batch % 100 == 0:\n",
    "                print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, loss))\n",
    " \n",
    "    if (epoch + 1) % 3 == 0:\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNITS: 2048\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Word2VecKeyedVectors' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-3d445bb312c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0minput_eval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_eval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_eval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"PREDICTIONS\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Word2VecKeyedVectors' object is not callable"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = '.\\\\training_checkpoints_wordstat_small2048'\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "current_word = \"мишка\"\n",
    "input_eval = [word2idx[current_word]]\n",
    "input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "print(\"UNITS: %s\" %(units))\n",
    "hidden = [tf.zeros((1, units))]\n",
    "predicted_id = word2idx[current_word]\n",
    "text_generated = current_word\n",
    "\n",
    "for i in range(4):\n",
    "    current_word = idx2word[predicted_id]\n",
    "    input_eval = [word2idx[current_word]]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)    \n",
    "    \n",
    "    predictions, hidden = model(input_eval, hidden)\n",
    "    \n",
    "    print(\"PREDICTIONS\")\n",
    "    print(predictions)\n",
    "    \n",
    "    predicted_id = tf.argmax(predictions[-1]).numpy()\n",
    "\n",
    "    text_generated += \" \" + idx2word[predicted_id]\n",
    "\n",
    "    print(text_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
