{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pymorphy2\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import unidecode\n",
    "from keras_preprocessing.text import Tokenizer\n",
    " \n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we load \n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\"G:\\\\New folder\\\\ruwikiruscorpora_tokens_elmo_1024_2019\\\\ruwikiruscorpora_upos_skipgram_300_2_2019\\\\model.bin\", binary=True)\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='холодный', tag=OpencorporaTag('ADJF,Qual masc,sing,nomn'), normal_form='холодный', score=0.5, methods_stack=((<DictionaryAnalyzer>, 'холодный', 86, 0),)),\n",
       " Parse(word='холодный', tag=OpencorporaTag('ADJF,Qual inan,masc,sing,accs'), normal_form='холодный', score=0.5, methods_stack=((<DictionaryAnalyzer>, 'холодный', 86, 4),))]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph.parse('холодный')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('лед_NOUN', 0.7199968099594116),\n",
       " ('снежный_ADJ', 0.6809204816818237),\n",
       " ('снег_NOUN', 0.6226842403411865),\n",
       " ('холодный_ADJ', 0.6221423745155334),\n",
       " ('льдистый_ADJ', 0.6192911863327026),\n",
       " ('льдинка_NOUN', 0.6153559684753418),\n",
       " ('сугроб_NOUN', 0.5984904766082764),\n",
       " ('холод_NOUN', 0.594094455242157),\n",
       " ('оледенелый_ADJ', 0.5915398001670837),\n",
       " ('сосулька_NOUN', 0.5900148749351501),\n",
       " ('торос_NOUN', 0.5898441076278687),\n",
       " ('замерзнуть_VERB', 0.5872437953948975),\n",
       " ('иней_NOUN', 0.5848214626312256),\n",
       " ('глыба_NOUN', 0.5659131407737732),\n",
       " ('замерзать_VERB', 0.5654045939445496),\n",
       " ('морозный_ADJ', 0.5636944770812988),\n",
       " ('обледенелый_ADJ', 0.563029408454895),\n",
       " ('студеный_ADJ', 0.5602612495422363),\n",
       " ('снеговой_ADJ', 0.5562052130699158),\n",
       " ('полынья_NOUN', 0.5557663440704346)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(['ледяной_ADJ'], topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_neighbour(word, pos, gend='masc'):\n",
    "    word = word.replace('ё', 'е')\n",
    "    lex = word + '_' + cotags[pos]\n",
    "    if lex in model:\n",
    "        neighbs = model.most_similar([lex], topn=20)\n",
    "        for nei in neighbs:\n",
    "            lex_n, ps_n = nei[0].split('_')\n",
    "            if '::' in lex_n:\n",
    "                continue\n",
    "            if cotags[pos] == ps_n:\n",
    "                if pos == 'NOUN':\n",
    "                    parse_result = morph.parse(lex_n)\n",
    "                    for ana in parse_result:\n",
    "                        if ana.normal_form == lex_n:\n",
    "                            if ana.tag.gender == gend:\n",
    "                                return lex_n\n",
    "                elif cotags[pos] == 'VERB' and word[-2:] == 'ся':\n",
    "                    if lex_n[-2:] == 'ся':\n",
    "                        return lex_n\n",
    "                elif cotags[pos] == 'VERB' and word[-2:] != 'ся':\n",
    "                    if lex_n[-2:] != 'ся':\n",
    "                        return lex_n\n",
    "                else:\n",
    "                    return lex_n\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flection(lex_neighb, tags):\n",
    "    tags = str(tags)\n",
    "    tags = re.sub(',[AGQSPMa-z-]+? ', ',', tags)\n",
    "    tags = tags.replace(\"impf,\", \"\")\n",
    "    tags = re.sub('([A-Z]) (plur|masc|femn|neut|inan)', '\\\\1,\\\\2', tags)\n",
    "    tags = tags.replace(\"Impe neut\", \"\")\n",
    "    tags = tags.split(',')\n",
    "    tags_clean = []\n",
    "    for t in tags:\n",
    "        if t:\n",
    "            if ' ' in t:\n",
    "                t1, t2 = t.split(' ')\n",
    "                t = t2\n",
    "            tags_clean.append(t)\n",
    "    tags = frozenset(tags_clean)\n",
    "    prep_for_gen = morph.parse(lex_neighb)\n",
    "    ana_array = []\n",
    "    for ana in prep_for_gen:\n",
    "        if ana.normal_form == lex_neighb:\n",
    "            ana_array.append(ana)\n",
    "    for ana in ana_array:\n",
    "        try:\n",
    "            flect = ana.inflect(tags)\n",
    "        except:\n",
    "            print(tags)\n",
    "            return None\n",
    "        if flect:\n",
    "            word_to_replace = flect.word\n",
    "            return word_to_replace\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cotags = {\n",
    "    'ADJF':'ADJ', # pymorphy2: word2vec \n",
    "    'ADJS' : 'ADJ', \n",
    "    'ADVB' : 'ADV', \n",
    "    'COMP' : 'ADV', \n",
    "    'GRND' : 'VERB', \n",
    "    'INFN' : 'VERB', \n",
    "    'NOUN' : 'NOUN', \n",
    "    'PRED' : 'ADV', \n",
    "    'PRTF' : 'ADJ', \n",
    "    'PRTS' : 'VERB', \n",
    "    'VERB' : 'VERB'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"G:\\\\New folder\\\\month-2011-12-qtraf_small\"\n",
    " \n",
    "text = open(file_path).read()\n",
    "#print(text)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([text])\n",
    " \n",
    "encoded = tokenizer.texts_to_sequences([text])[0]\n",
    "#print(encoded) \n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "#print(\"vocab_size %s\" %(vocab_size))\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = tokenizer.index_word\n",
    "#print(\"word_index %s\" %( tokenizer.word_index))\n",
    "#print(\"index_word %s\" %( tokenizer.index_word))\n",
    "\n",
    "sequences = list()\n",
    "\n",
    "for i in range(1, len(encoded)):\n",
    "    sequence = encoded[i - 1:i + 1]\n",
    "    sequences.append(sequence)\n",
    "sequences = np.array(sequences)\n",
    "#print(word2idx)\n",
    "X, Y = sequences[:, 0], sequences[:, 1]\n",
    "X = np.expand_dims(X, 1)\n",
    "Y = np.expand_dims(Y, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['линя', 'контур', 'электропередача', 'направление', 'зигзаг', 'полоса', 'пунктир', 'вертикаль', 'горизонталя', 'плоскость', 'точка']\n",
      "['направление', 'полоса', 'плоскость', 'точка']\n"
     ]
    }
   ],
   "source": [
    "#This function returns only similar words that contains in train dataset\n",
    "def sortSimilarListByDataset(words_list):\n",
    "    ret_list = []\n",
    "    for word in words_list:\n",
    "        try:\n",
    "            if word2idx[word]:\n",
    "                ret_list.append(word)\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return ret_list\n",
    "#Returns Top N words, that similars with\n",
    "def getSimilarsForWord(word, top=10):\n",
    "    parsed = morph.parse(word)\n",
    "    pos = cotags[parsed[0].tag.POS]\n",
    "    gensim_find_word = word + \"_\" + pos\n",
    "    most_similars = model.most_similar([gensim_find_word], topn=top)\n",
    "    return_list = []\n",
    "    for sim in most_similars:\n",
    "        sim_parsed = sim[0].split(\"_\")\n",
    "        if sim_parsed[1] == pos:\n",
    "            return_list.append(sim_parsed[0])\n",
    "    return return_list\n",
    "my_word = \"линия\"\n",
    "#find similars\n",
    "sim_list = getSimilarsForWord(my_word, 40)\n",
    "print(sim_list)\n",
    "dataset_list = sortSimilarListByDataset(sim_list)\n",
    "print(dataset_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 100\n",
    "BATCH_SIZE = 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X, Y)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "embedding_dim = 100\n",
    "units = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, units, batch_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.units = units\n",
    "        self.batch_size = batch_size\n",
    " \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    " \n",
    "        self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_activation='sigmoid',\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    " \n",
    "    def call(self, inputs, hidden):\n",
    "        inputs = self.embedding(inputs)\n",
    "        #print(inputs)\n",
    "        output, states = self.gru(inputs, initial_state=hidden)\n",
    " \n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    " \n",
    "        x = self.fc(output)\n",
    " \n",
    "        return x, states\n",
    "\n",
    "keras_model = Model(vocab_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    " \n",
    "#checkpoint_dir = '.\\\\training_checkpoints_wordstat'\n",
    "checkpoint_dir = '.\\\\training_checkpoints_wordstat_small2048'\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=keras_model)\n",
    "\n",
    "def loss_function(labels, logits):\n",
    "    return tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "EPOCHS = 10\n",
    "# for epoch in range(EPOCHS):\n",
    "#     start = time.time()\n",
    " \n",
    "#     hidden = keras_model.reset_states()\n",
    " \n",
    "#     for (batch, (input, target)) in enumerate(dataset):\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             predictions, hidden = model(input, hidden)\n",
    " \n",
    "#             target = tf.reshape(target, (-1,))\n",
    "#             loss = loss_function(target, predictions)\n",
    " \n",
    "#             grads = tape.gradient(loss, keras_model.variables)\n",
    "#             optimizer.apply_gradients(zip(grads, keras_model.variables))\n",
    " \n",
    "#             if batch % 100 == 0:\n",
    "#                 print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, loss))\n",
    " \n",
    "#     if (epoch + 1) % 3 == 0:\n",
    "#         checkpoint.save(file_prefix=checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNITS: 2048\n",
      "dataset_words_list ['медвежонок', 'мишка']\n",
      "[['медвежонок'], ['мишка']]\n",
      "медвежонок колки шва бабышек газового\n",
      "мишка окл информатики пиона секущей\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = '.\\\\training_checkpoints_wordstat_small2048'\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "current_word = \"мишка\"\n",
    "input_eval = [word2idx[current_word]]\n",
    "input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "print(\"UNITS: %s\" %(units))\n",
    "hidden = [tf.zeros((1, units))]\n",
    "\n",
    "#Now we find similars for start word\n",
    "similar_words = getSimilarsForWord(current_word, 10)\n",
    "similar_words.append(current_word)\n",
    "dataset_words_list = sortSimilarListByDataset(similar_words)\n",
    "print(\"dataset_words_list %s\" %(dataset_words_list))\n",
    "\n",
    "sequences_lists = [[word] for word in dataset_words_list]\n",
    "print(sequences_lists)\n",
    "for sequence in sequences_lists:\n",
    "    for i in range(4):\n",
    "        input_eval = [word2idx[sequence[i]]]\n",
    "        input_eval = tf.expand_dims(input_eval, 0)    \n",
    "\n",
    "        predictions, hidden = keras_model(input_eval, hidden)\n",
    "#         print(\"PREDICTIONS\")\n",
    "#         print(predictions)\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[-1]).numpy()\n",
    "\n",
    "        sequence.append(idx2word[predicted_id])\n",
    "        \n",
    "for sequence in sequences_lists:\n",
    "    print(\" \".join(sequence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
